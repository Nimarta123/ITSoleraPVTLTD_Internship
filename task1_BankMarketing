import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report, f1_score, roc_curve, auc

df = pd.read_csv('bank.csv')

binary_cols = ['default', 'housing', 'loan', 'deposit']
le = LabelEncoder()
for col in binary_cols:
    df[col] = le.fit_transform(df[col])


categorical_cols = ['job', 'marital', 'education', 'contact', 'month', 'poutcome']
df_final = pd.get_dummies(df, columns=categorical_cols)


X = df_final.drop('deposit', axis=1)
y = df_final['deposit']


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


scaler = StandardScaler()
X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X.columns)
X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X.columns)


# Logistic Regression (Linear Model)
lr_model = LogisticRegression(max_iter=1000)
lr_model.fit(X_train_scaled, y_train)
lr_preds = lr_model.predict(X_test_scaled)


rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
rf_preds = rf_model.predict(X_test)


print(f"Logistic Regression F1: {f1_score(y_test, lr_preds):.3f}")
print(f"Random Forest F1: {f1_score(y_test, rf_preds):.3f}")


fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
sns.heatmap(confusion_matrix(y_test, lr_preds), annot=True, fmt='d', cmap='Blues', ax=ax1)
ax1.set_title('Logistic Regression CM')
sns.heatmap(confusion_matrix(y_test, rf_preds), annot=True, fmt='d', cmap='Greens', ax=ax2)
ax2.set_title('Random Forest CM')
plt.savefig('confusion_matrices.png')


lr_probs = lr_model.predict_proba(X_test_scaled)[:, 1]
rf_probs = rf_model.predict_proba(X_test)[:, 1]
fpr_lr, tpr_lr, _ = roc_curve(y_test, lr_probs)
fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_probs)

plt.figure(figsize=(8, 6))
plt.plot(fpr_lr, tpr_lr, label=f'LR (AUC={auc(fpr_lr, tpr_lr):.2f})')
plt.plot(fpr_rf, tpr_rf, label=f'RF (AUC={auc(fpr_rf, tpr_rf):.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.title('ROC Curves (Model Performance)')
plt.legend()
plt.savefig('roc_curves.png')


feat_importances = pd.Series(rf_model.feature_importances_, index=X.columns).sort_values(ascending=False).head(15)
plt.figure(figsize=(10, 6))
feat_importances.plot(kind='barh', color='skyblue')
plt.title('Global Feature Importance (Random Forest)')
plt.gca().invert_yaxis()
plt.savefig('feature_importance.png')


for i in range(5):
    sample = X_test_scaled.iloc[i]
    contributions = lr_model.coef_[0] * sample.values
    cont_df = pd.DataFrame({'Feature': X.columns, 'Impact': contributions})
    cont_df = cont_df.sort_values(by='Impact', key=abs, ascending=False).head(5)
    
    plt.figure(figsize=(8, 4))
    sns.barplot(x='Impact', y='Feature', data=cont_df, palette='vlag')
    plt.title(f'Customer {i+1} Explanation (Pred: {"Yes" if lr_preds[i] else "No"})')
    plt.savefig(f'explanation_customer_{i+1}.png')
